{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2.4 API / Scraping\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# a helper function\n",
    "\n",
    "my_string = \"blahblah=14\"\n",
    "\n",
    "import re\n",
    "\n",
    "def get_num(a_string):\n",
    "    \n",
    "    m = re.search(\"\\d\", a_string)\n",
    "    return a_string[m.start():]\n",
    "\n",
    "print(get_num(my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class ESSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"ESS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://portland.craigslist.org/d/jobs/search/jjj',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every word class element on the page.\n",
    "        response.selector.remove_namespaces()\n",
    "        \n",
    "        for article in response.xpath('//li[@class=\"result-row\"]'): \n",
    "     \n",
    "                # Yield a dictionary with the values we want.\n",
    "                yield {\n",
    "                    'date': article.xpath('*/time/@datetime').extract(), # this worked as //time\n",
    "                    'job': article.xpath('*/a[@class=\"result-title hdrlnk\"]/text()').extract(), # also worked as //a\n",
    "                    'location': article.xpath('p/span/span[@class=\"result-hood\"]/text()').extract(),\n",
    "                }\n",
    "            # Get the URL of the next page.\n",
    "        next_page = response.xpath('//div/div/span/a[@class=\"button next\"]/@href').extract_first()\n",
    "        \n",
    "        #print(get_num(next_page))  # this was just a double check to make sure it was running\n",
    "        \n",
    "        # there are 119 results on each page, so the url next_page ends in a multiple of 120\n",
    "        # since we don't want to go beyond page 9 let's calc 120 x 9\n",
    "        \n",
    "        maximum  = 9 * 120  \n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "        if next_page is not None and int(get_num(next_page)) < maximum:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'data3.json',       # Name our storage file.\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(ESSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 3)\n",
      "                 date                                                job  \\\n",
      "0  [2018-10-05 21:46]                            [Bartender at Taphouse]   \n",
      "1  [2018-10-05 20:36]  [►MAKE UP TO $25/HOUR ON YOUR OWN SCHEDULE /W ...   \n",
      "2  [2018-10-05 20:34]                                [EXTRA CASH DRIVER]   \n",
      "3  [2018-10-05 20:14]     [Recptionist/Optical assistant for Eye Clinic]   \n",
      "4  [2018-10-05 20:11]                        [Legal Secretary/Assistant]   \n",
      "\n",
      "                  location  \n",
      "0  [ (9230 SW Burnham St)]  \n",
      "1                       []  \n",
      "2        [ (VANCOUVER,WA)]  \n",
      "3           [ (Milwaukie)]  \n",
      "4            [ (Downtown)]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data from all 9 pages\n",
    "ESSdf = pd.read_json('data3.json', orient='records')\n",
    "print(ESSdf.shape)\n",
    "print(ESSdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little bit of a mess here. Cleaning up the data a little bit...\n",
    "\n",
    "First, it looks like each cell is a 1 item list, so I'm going to fix it so each cell is just the item itself, not the list. <br />\n",
    "I used [this resource](https://chrisalbon.com/python/data_wrangling/pandas_expand_cells_containing_lists/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ESSdf.columns:\n",
    "    # separate the list into columns (in this case, 1 each)\n",
    "    temporary = ESSdf[col].apply(pd.Series)\n",
    "    ESSdf[col] = temporary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a little bit more cleaning, so we can actually analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date to datetime format\n",
    "\n",
    "ESSdf['date'] = pd.to_datetime(ESSdf['date'], infer_datetime_format=True, errors=\"ignore\")\n",
    "\n",
    "# take out the time\n",
    "ESSdf['time posted'] = ESSdf['date'].dt.time\n",
    "\n",
    "#also take out the date\n",
    "ESSdf['date posted'] = ESSdf['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the job titles a tiny bit\n",
    "\n",
    "# make everything lowercase\n",
    "ESSdf['job'] = ESSdf['job'].str.lower()\n",
    "\n",
    "# get rid of some special characters\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "ESSdf['job'] = ESSdf['job'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns \n",
    "cols = ['date', 'date posted', 'time posted', 'location', 'job']\n",
    "\n",
    "jobs = ESSdf[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date posted</th>\n",
       "      <th>time posted</th>\n",
       "      <th>location</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-05 21:46:00</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>21:46:00</td>\n",
       "      <td>(9230 SW Burnham St)</td>\n",
       "      <td>bartender at taphouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-05 20:36:00</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>20:36:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>►make up to $25/hour on your own schedule /w p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-05 20:34:00</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>20:34:00</td>\n",
       "      <td>(VANCOUVER,WA)</td>\n",
       "      <td>extra cash driver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-05 20:14:00</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>20:14:00</td>\n",
       "      <td>(Milwaukie)</td>\n",
       "      <td>recptionist/optical assistant for eye clinic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-05 20:11:00</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>20:11:00</td>\n",
       "      <td>(Downtown)</td>\n",
       "      <td>legal secretary/assistant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date date posted time posted               location  \\\n",
       "0 2018-10-05 21:46:00  2018-10-05    21:46:00   (9230 SW Burnham St)   \n",
       "1 2018-10-05 20:36:00  2018-10-05    20:36:00                    NaN   \n",
       "2 2018-10-05 20:34:00  2018-10-05    20:34:00         (VANCOUVER,WA)   \n",
       "3 2018-10-05 20:14:00  2018-10-05    20:14:00            (Milwaukie)   \n",
       "4 2018-10-05 20:11:00  2018-10-05    20:11:00             (Downtown)   \n",
       "\n",
       "                                                 job  \n",
       "0                              bartender at taphouse  \n",
       "1  ►make up to $25/hour on your own schedule /w p...  \n",
       "2                                  extra cash driver  \n",
       "3       recptionist/optical assistant for eye clinic  \n",
       "4                          legal secretary/assistant  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at which days the postings are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['day'] = jobs['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(jobs['day'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like they are all from just 2 days, most of which from one. Let's look at the job titles instead, and get a list of the most common job topics using some NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "jobs['parsed'] = jobs['job'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_of_words(text, all_words):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    all_words += ([token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop])\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(all_words).most_common(1000)]\n",
    "    \n",
    "all_words = []\n",
    "jobs['parsed'].apply(bag_of_words, args=(all_words,))\n",
    "\n",
    "common_words = [item[0] for item in Counter(all_words).most_common(150)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most common words and then move on to getting the main topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$',\n",
       " 'time',\n",
       " 'driver',\n",
       " 'hire',\n",
       " 'to',\n",
       " 'part',\n",
       " 'need',\n",
       " 'assistant',\n",
       " 'be',\n",
       " 'for',\n",
       " 'and',\n",
       " 'a',\n",
       " '-PRON-',\n",
       " 'make',\n",
       " 'cdl',\n",
       " 'in',\n",
       " 'bonus',\n",
       " 'want',\n",
       " 'full',\n",
       " 'warehouse']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Creating the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "topic_paras_tfidf=vectorizer.fit_transform(all_words)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=chosenlist\n",
    "    return(topwords)\n",
    "\n",
    "svd= TruncatedSVD(3)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "topic_paras_lsa = lsa.fit_transform(topic_paras_tfidf)\n",
    "\n",
    "components_lsa = word_topic(topic_paras_tfidf, topic_paras_lsa, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         time 128.0\n",
       "0        bonus 25.22\n",
       "0    associate 24.47\n",
       "0     portland 21.37\n",
       "0         auto 12.85\n",
       "1       driver 105.0\n",
       "1          make 48.4\n",
       "1        night 45.14\n",
       "1         pron 43.53\n",
       "1          cdl 38.82\n",
       "2          hire 90.0\n",
       "2         need 75.07\n",
       "2            hr 48.5\n",
       "2        amazon 47.5\n",
       "2         sale 39.11\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words(components_lsa, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the most common words for the most part overlapped completely with the main topics. It is probably because of the very small text sample size and the lack of context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
